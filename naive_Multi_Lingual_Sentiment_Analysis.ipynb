{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 93282,
          "databundleVersionId": 11098970,
          "sourceType": "competition"
        },
        {
          "sourceId": 104449,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 68809,
          "modelId": 91102
        }
      ],
      "dockerImageVersionId": 30886,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "0pL4SfOEpaWM"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "multi_lingual_sentiment_analysis_path = kagglehub.competition_download('multi-lingual-sentiment-analysis')\n",
        "metaresearch_llama_3_1_transformers_8b_instruct_2_path = kagglehub.model_download('metaresearch/llama-3.1/Transformers/8b-instruct/2')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "gOc9I-4fpaWN"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T09:53:00.91418Z",
          "iopub.execute_input": "2025-02-17T09:53:00.914523Z",
          "iopub.status.idle": "2025-02-17T09:53:00.925663Z",
          "shell.execute_reply.started": "2025-02-17T09:53:00.914495Z",
          "shell.execute_reply": "2025-02-17T09:53:00.92489Z"
        },
        "collapsed": true,
        "id": "u1Fv26nZpaWN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch transformers peft datasets accelerate sentencepiece"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T09:53:00.953754Z",
          "iopub.execute_input": "2025-02-17T09:53:00.954032Z",
          "iopub.status.idle": "2025-02-17T09:53:04.620155Z",
          "shell.execute_reply.started": "2025-02-17T09:53:00.954008Z",
          "shell.execute_reply": "2025-02-17T09:53:04.619265Z"
        },
        "_kg_hide-output": false,
        "_kg_hide-input": false,
        "collapsed": true,
        "id": "tyA44hgcpaWO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T09:53:04.621628Z",
          "iopub.execute_input": "2025-02-17T09:53:04.621952Z",
          "iopub.status.idle": "2025-02-17T09:53:08.251918Z",
          "shell.execute_reply.started": "2025-02-17T09:53:04.62192Z",
          "shell.execute_reply": "2025-02-17T09:53:08.251007Z"
        },
        "collapsed": true,
        "id": "ZHBGMgD9paWO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer,BitsAndBytesConfig\n",
        "from datasets import load_dataset,DatasetDict\n",
        "from peft import LoraConfig, get_peft_model"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T09:53:08.254142Z",
          "iopub.execute_input": "2025-02-17T09:53:08.254411Z",
          "iopub.status.idle": "2025-02-17T09:53:08.258659Z",
          "shell.execute_reply.started": "2025-02-17T09:53:08.254389Z",
          "shell.execute_reply": "2025-02-17T09:53:08.257795Z"
        },
        "id": "CLQX0d0ipaWP"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T09:53:08.259817Z",
          "iopub.execute_input": "2025-02-17T09:53:08.260011Z",
          "iopub.status.idle": "2025-02-17T09:53:08.275811Z",
          "shell.execute_reply.started": "2025-02-17T09:53:08.259995Z",
          "shell.execute_reply": "2025-02-17T09:53:08.275104Z"
        },
        "id": "lt62IJ2OpaWP"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"/kaggle/input/llama-3.1/transformers/8b-instruct/2\"\n",
        "# model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16)\n",
        "\n",
        "# print(\"Model loaded successfully in full precision!\")\n",
        "# print(10*'_',f\"Base Model size: {model.get_memory_footprint():,} bytes\\n\")\n",
        "# Apply 4-bit quantization\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")\n",
        "\n",
        "# Reload the model with quantization\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, low_cpu_mem_usage=True)\n",
        "\n",
        "print(\"Model successfully quantized to 4-bit!\")\n",
        "print(10*'_',f\"Base Model size: {model.get_memory_footprint():,} bytes\\n\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T09:53:08.276606Z",
          "iopub.execute_input": "2025-02-17T09:53:08.276806Z",
          "iopub.status.idle": "2025-02-17T09:53:25.712151Z",
          "shell.execute_reply.started": "2025-02-17T09:53:08.276788Z",
          "shell.execute_reply": "2025-02-17T09:53:25.711218Z"
        },
        "id": "DJnNUIsYpaWP",
        "outputId": "97d99604-bfe2-4189-ca79-028d402d6b8e",
        "colab": {
          "referenced_widgets": [
            "87ab3a3477194cfa8ffcf2f287ced7a9"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "87ab3a3477194cfa8ffcf2f287ced7a9"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Model successfully quantized to 4-bit!\n__________ Base Model size: 5,591,548,160 bytes\n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# model.to(\"cuda\")\n",
        "model.to('cuda:0')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T09:53:25.713045Z",
          "iopub.execute_input": "2025-02-17T09:53:25.713377Z",
          "iopub.status.idle": "2025-02-17T09:53:25.728675Z",
          "shell.execute_reply.started": "2025-02-17T09:53:25.713329Z",
          "shell.execute_reply": "2025-02-17T09:53:25.72779Z"
        },
        "id": "ho89qXkGpaWQ",
        "outputId": "60e636af-c285-468c-f414-1668df4931ef"
      },
      "outputs": [
        {
          "execution_count": 69,
          "output_type": "execute_result",
          "data": {
            "text/plain": "LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(128256, 4096)\n    (layers): ModuleList(\n      (0-31): 32 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=load_dataset('csv',data_files='/kaggle/input/multi-lingual-sentiment-analysis/train.csv')\n",
        "\n",
        "# Split dataset (90% Train, 10% Validation)\n",
        "dataset = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
        "\n",
        "# Rename splits for clarity\n",
        "dataset = DatasetDict({\n",
        "    \"train\": dataset[\"train\"],\n",
        "    \"validation\": dataset[\"test\"]  # Treat this as validation\n",
        "})\n",
        "\n",
        "print(dataset)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T09:53:25.729625Z",
          "iopub.execute_input": "2025-02-17T09:53:25.72993Z",
          "iopub.status.idle": "2025-02-17T09:53:25.958493Z",
          "shell.execute_reply.started": "2025-02-17T09:53:25.729898Z",
          "shell.execute_reply": "2025-02-17T09:53:25.957705Z"
        },
        "id": "tyWtgT44paWQ",
        "outputId": "aae84af3-7913-48ec-f0bd-1d438e025ad8"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "DatasetDict({\n    train: Dataset({\n        features: ['ID', 'sentence', 'label', 'language'],\n        num_rows: 900\n    })\n    validation: Dataset({\n        features: ['ID', 'sentence', 'label', 'language'],\n        num_rows: 100\n    })\n})\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "DoL9e1YxpaWR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def format_prompt(example):\n",
        "    prompt = f\"Classify the sentiment of the following language {example['language']} text: '{example['text']}' \\n Sentiment:\"\n",
        "    return {\"prompt\": prompt, \"labels\": example[\"label\"]}\n",
        "\n",
        "tokenized_datasets = dataset.map(format_prompt)\n",
        "tokenized_datasets = tokenized_datasets.remove_columns([\"text\", \"id\", \"language\"])\n",
        "tokenized_datasets.set_format(\"torch\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T09:53:25.960418Z",
          "iopub.execute_input": "2025-02-17T09:53:25.960632Z",
          "iopub.status.idle": "2025-02-17T09:53:25.969381Z",
          "shell.execute_reply.started": "2025-02-17T09:53:25.960614Z",
          "shell.execute_reply": "2025-02-17T09:53:25.968735Z"
        },
        "id": "E_yND3NgpaWR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T09:53:25.970433Z",
          "iopub.execute_input": "2025-02-17T09:53:25.970714Z",
          "iopub.status.idle": "2025-02-17T09:53:25.981921Z",
          "shell.execute_reply.started": "2025-02-17T09:53:25.970678Z",
          "shell.execute_reply": "2025-02-17T09:53:25.9812Z"
        },
        "id": "GO99maaypaWR",
        "outputId": "08b3342f-d29a-4380-bc47-b72d66f3be06"
      },
      "outputs": [
        {
          "execution_count": 72,
          "output_type": "execute_result",
          "data": {
            "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['ID', 'sentence', 'label', 'language', 'prompt'],\n        num_rows: 900\n    })\n    validation: Dataset({\n        features: ['ID', 'sentence', 'label', 'language', 'prompt'],\n        num_rows: 100\n    })\n})"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train'][:1]['prompt']"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T09:53:25.982849Z",
          "iopub.execute_input": "2025-02-17T09:53:25.98314Z",
          "iopub.status.idle": "2025-02-17T09:53:25.998623Z",
          "shell.execute_reply.started": "2025-02-17T09:53:25.983108Z",
          "shell.execute_reply": "2025-02-17T09:53:25.997879Z"
        },
        "id": "cupmJDyFpaWR",
        "outputId": "9313d684-73d1-4ad9-d2af-0d9643bcd1da"
      },
      "outputs": [
        {
          "execution_count": 73,
          "output_type": "execute_result",
          "data": {
            "text/plain": "[\"Below is a sentence in an Indian language. Classify its sentiment.\\nSentence: 'ਮੈਂ ਬਹੁਤ ਲੰਬੇ ਸਮੇਂ ਤੋਂ ਲੈਕਮੇ ਉਤਪਾਦਾਂ ਦੀ ਵਰਤੋਂ ਕਰ ਰਿਹਾ/ਰਹੀ ਹਾਂ ਅਤੇ ਇਹ ਮੇਰੀ ਮੇਕ-ਅੱਪ ਕਿੱਟ ਵਿੱਚ ਹੁਣੇ ਆਇਆ ਹੈ ਜੋ ਇਸਦੀ ਬ੍ਰਾਂਡ ਇਮੇਜ ਅਨੁਸਾਰ ਖਰਾ ਉਤਰਦਾ ਹੈ। ਇਹ ਮੇਰੀ ਚਮੜੀ ਦੇ ਕਾਲੇ ਘੇਰਿਆਂ ਨੂੰ ਢੱਕਦਾ ਹੈ, ਇਸ ਨੂੰ ਚਮਕਦਾਰ ਬਣਾਉਂਦਾ ਹੈ ਅਤੇ ਇਕਸਾਰ ਟੋਨ ਦਿੰਦਾ।'\\nSentiment:\"]"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Available VRAM:\", torch.cuda.get_device_properties(0).total_memory / 1e9, \"GB\")\n",
        "print(\"Torch Reserved Memory:\", torch.cuda.memory_reserved() / 1e9, \"GB\")\n",
        "print(\"Torch Allocated Memory:\", torch.cuda.memory_allocated() / 1e9, \"GB\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T09:53:25.999601Z",
          "iopub.execute_input": "2025-02-17T09:53:25.999896Z",
          "iopub.status.idle": "2025-02-17T09:53:26.015566Z",
          "shell.execute_reply.started": "2025-02-17T09:53:25.999875Z",
          "shell.execute_reply": "2025-02-17T09:53:26.014766Z"
        },
        "id": "RreuUBILpaWR",
        "outputId": "aa80abd4-c6ab-48ad-8cd3-76905c852e99"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Available VRAM: 15.828320256 GB\nTorch Reserved Memory: 11.549016064 GB\nTorch Allocated Memory: 11.435756032 GB\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Ensure padding is handled"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T09:53:26.016318Z",
          "iopub.execute_input": "2025-02-17T09:53:26.016622Z",
          "iopub.status.idle": "2025-02-17T09:53:26.534146Z",
          "shell.execute_reply.started": "2025-02-17T09:53:26.0166Z",
          "shell.execute_reply": "2025-02-17T09:53:26.533234Z"
        },
        "id": "L1rBbWj-paWS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# LoRA Configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=8,  # Low-rank dimension\n",
        "    lora_alpha=16,  # Scaling factor\n",
        "    lora_dropout=0.1,  # Prevents overfitting\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],  # Optimizes attention layers\n",
        "    # bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# Apply LoRA\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "# trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "# total_params = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "# print(f\"Trainable Parameters: {trainable_params:,}\")\n",
        "# print(f\"Total Parameters: {total_params:,}\")\n",
        "# print(f\"Percentage of Trainable Parameters: {100 * trainable_params / total_params:.4f}%\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T09:53:26.535022Z",
          "iopub.execute_input": "2025-02-17T09:53:26.535312Z",
          "iopub.status.idle": "2025-02-17T09:53:26.654875Z",
          "shell.execute_reply.started": "2025-02-17T09:53:26.535283Z",
          "shell.execute_reply": "2025-02-17T09:53:26.654154Z"
        },
        "id": "Bg8fW3PUpaWS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "# Tokenization Function\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"prompt\"], padding=\"max_length\", truncation=True, max_length=256)\n",
        "\n",
        "# Tokenize Dataset\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Data Collator for Padding & Masking\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False  # We are doing causal LM fine-tuning\n",
        ")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T09:53:26.655648Z",
          "iopub.execute_input": "2025-02-17T09:53:26.655969Z",
          "iopub.status.idle": "2025-02-17T09:53:27.037085Z",
          "shell.execute_reply.started": "2025-02-17T09:53:26.655937Z",
          "shell.execute_reply": "2025-02-17T09:53:27.036425Z"
        },
        "id": "H3yaH1Y8paWS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_train_batch_size=1,  # Batch size for each GPU\n",
        "    per_device_eval_batch_size=1,   # Batch size for evaluation\n",
        "    gradient_accumulation_steps=8,  # Simulates batch_size=16\n",
        "    evaluation_strategy=\"steps\",    # Evaluate after each set of steps\n",
        "    eval_steps=500,                 # Evaluate every 500 steps (adjust as needed)\n",
        "    save_strategy=\"steps\",         # Save model at each evaluation step\n",
        "    save_steps=500,                # Save checkpoint every 500 steps\n",
        "    save_total_limit=2,            # Keep only the 2 most recent checkpoints\n",
        "    num_train_epochs=1,            # Number of epochs to train\n",
        "    logging_dir=\"./logs\",          # Log directory for training\n",
        "    logging_steps=100,             # Log every 100 steps\n",
        "    report_to=\"none\",              # Disable reporting to other platforms\n",
        "    fp16=True,                     # Enable mixed precision\n",
        "    optim=\"adamw_torch\",           # Optimizer choice\n",
        "    lr_scheduler_type=\"cosine\",    # Cosine decay for better convergence\n",
        "    warmup_ratio=0.03,             # Warm-up ratio\n",
        "    weight_decay=0.01,             # Regularization\n",
        "    load_best_model_at_end=True,   # Load the best model based on eval loss\n",
        "    push_to_hub=False              # Disable model uploading\n",
        ")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T09:53:27.037853Z",
          "iopub.execute_input": "2025-02-17T09:53:27.038128Z",
          "iopub.status.idle": "2025-02-17T09:53:27.073393Z",
          "shell.execute_reply.started": "2025-02-17T09:53:27.038101Z",
          "shell.execute_reply": "2025-02-17T09:53:27.072547Z"
        },
        "id": "IdaDfGEKpaWS",
        "outputId": "83b2db70-3a04-485b-ae1d-7fee4e85b51a"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if GPU is available\n",
        "print(\"CUDA Available:\", torch.cuda.is_available())  # Should print True if GPU is available\n",
        "print(\"Using Device:\", torch.cuda.get_device_name(0))  # Should print the GPU model (A100, T4, etc.)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T09:53:27.074436Z",
          "iopub.execute_input": "2025-02-17T09:53:27.074758Z",
          "iopub.status.idle": "2025-02-17T09:53:27.080052Z",
          "shell.execute_reply.started": "2025-02-17T09:53:27.074728Z",
          "shell.execute_reply": "2025-02-17T09:53:27.079189Z"
        },
        "id": "7mcZ6079paWT",
        "outputId": "dea21cd4-8200-4c90-b3d0-af25f8146fb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "CUDA Available: True\nUsing Device: Tesla T4\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "# Start Training with Step-wise Evaluation\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T09:53:27.080962Z",
          "iopub.execute_input": "2025-02-17T09:53:27.081213Z",
          "iopub.status.idle": "2025-02-17T09:53:28.274289Z",
          "shell.execute_reply.started": "2025-02-17T09:53:27.08118Z",
          "shell.execute_reply": "2025-02-17T09:53:28.272875Z"
        },
        "id": "4XVEU2RHpaWT"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}