{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 93282,
          "databundleVersionId": 11098970,
          "sourceType": "competition"
        },
        {
          "sourceId": 104449,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 68809,
          "modelId": 91102
        }
      ],
      "dockerImageVersionId": 30886,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "0pL4SfOEpaWM"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "multi_lingual_sentiment_analysis_path = kagglehub.competition_download('multi-lingual-sentiment-analysis')\n",
        "metaresearch_llama_3_1_transformers_8b_instruct_2_path = kagglehub.model_download('metaresearch/llama-3.1/Transformers/8b-instruct/2')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "gOc9I-4fpaWN"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T09:53:00.91418Z",
          "iopub.execute_input": "2025-02-17T09:53:00.914523Z",
          "iopub.status.idle": "2025-02-17T09:53:00.925663Z",
          "shell.execute_reply.started": "2025-02-17T09:53:00.914495Z",
          "shell.execute_reply": "2025-02-17T09:53:00.92489Z"
        },
        "collapsed": true,
        "id": "u1Fv26nZpaWN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch transformers peft datasets accelerate sentencepiece"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T09:53:00.953754Z",
          "iopub.execute_input": "2025-02-17T09:53:00.954032Z",
          "iopub.status.idle": "2025-02-17T09:53:04.620155Z",
          "shell.execute_reply.started": "2025-02-17T09:53:00.954008Z",
          "shell.execute_reply": "2025-02-17T09:53:04.619265Z"
        },
        "_kg_hide-output": false,
        "_kg_hide-input": false,
        "collapsed": true,
        "id": "tyA44hgcpaWO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T09:53:04.621628Z",
          "iopub.execute_input": "2025-02-17T09:53:04.621952Z",
          "iopub.status.idle": "2025-02-17T09:53:08.251918Z",
          "shell.execute_reply.started": "2025-02-17T09:53:04.62192Z",
          "shell.execute_reply": "2025-02-17T09:53:08.251007Z"
        },
        "collapsed": true,
        "id": "ZHBGMgD9paWO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer,BitsAndBytesConfig\n",
        "from datasets import load_dataset,DatasetDict\n",
        "from peft import LoraConfig, get_peft_model"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T09:53:08.254142Z",
          "iopub.execute_input": "2025-02-17T09:53:08.254411Z",
          "iopub.status.idle": "2025-02-17T09:53:08.258659Z",
          "shell.execute_reply.started": "2025-02-17T09:53:08.254389Z",
          "shell.execute_reply": "2025-02-17T09:53:08.257795Z"
        },
        "id": "CLQX0d0ipaWP"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T09:53:08.259817Z",
          "iopub.execute_input": "2025-02-17T09:53:08.260011Z",
          "iopub.status.idle": "2025-02-17T09:53:08.275811Z",
          "shell.execute_reply.started": "2025-02-17T09:53:08.259995Z",
          "shell.execute_reply": "2025-02-17T09:53:08.275104Z"
        },
        "id": "lt62IJ2OpaWP"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"/kaggle/input/llama-3.1/transformers/8b-instruct/2\"\n",
        "# model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16)\n",
        "\n",
        "# print(\"Model loaded successfully in full precision!\")\n",
        "# print(10*'_',f\"Base Model size: {model.get_memory_footprint():,} bytes\\n\")\n",
        "# Apply 4-bit quantization\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")\n",
        "\n",
        "# Reload the model with quantization\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, low_cpu_mem_usage=True)\n",
        "\n",
        "print(\"Model successfully quantized to 4-bit!\")\n",
        "print(10*'_',f\"Base Model size: {model.get_memory_footprint():,} bytes\\n\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T09:53:08.276606Z",
          "iopub.execute_input": "2025-02-17T09:53:08.276806Z",
          "iopub.status.idle": "2025-02-17T09:53:25.712151Z",
          "shell.execute_reply.started": "2025-02-17T09:53:08.276788Z",
          "shell.execute_reply": "2025-02-17T09:53:25.711218Z"
        },
        "id": "DJnNUIsYpaWP",
        "outputId": "97d99604-bfe2-4189-ca79-028d402d6b8e",
        "colab": {
          "referenced_widgets": [
            "87ab3a3477194cfa8ffcf2f287ced7a9"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "87ab3a3477194cfa8ffcf2f287ced7a9"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Model successfully quantized to 4-bit!\n__________ Base Model size: 5,591,548,160 bytes\n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# model.to(\"cuda\")\n",
        "model.to('cuda:0')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T09:53:25.713045Z",
          "iopub.execute_input": "2025-02-17T09:53:25.713377Z",
          "iopub.status.idle": "2025-02-17T09:53:25.728675Z",
          "shell.execute_reply.started": "2025-02-17T09:53:25.713329Z",
          "shell.execute_reply": "2025-02-17T09:53:25.72779Z"
        },
        "id": "ho89qXkGpaWQ",
        "outputId": "60e636af-c285-468c-f414-1668df4931ef"
      },
      "outputs": [
        {
          "execution_count": 69,
          "output_type": "execute_result",
          "data": {
            "text/plain": "LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(128256, 4096)\n    (layers): ModuleList(\n      (0-31): 32 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=load_dataset('csv',data_files='/kaggle/input/multi-lingual-sentiment-analysis/train.csv')\n",
        "\n",
        "# Split dataset (90% Train, 10% Validation)\n",
        "dataset = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
        "\n",
        "# Rename splits for clarity\n",
        "dataset = DatasetDict({\n",
        "    \"train\": dataset[\"train\"],\n",
        "    \"validation\": dataset[\"test\"]  # Treat this as validation\n",
        "})\n",
        "\n",
        "print(dataset)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T09:53:25.729625Z",
          "iopub.execute_input": "2025-02-17T09:53:25.72993Z",
          "iopub.status.idle": "2025-02-17T09:53:25.958493Z",
          "shell.execute_reply.started": "2025-02-17T09:53:25.729898Z",
          "shell.execute_reply": "2025-02-17T09:53:25.957705Z"
        },
        "id": "tyWtgT44paWQ",
        "outputId": "aae84af3-7913-48ec-f0bd-1d438e025ad8"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "DatasetDict({\n    train: Dataset({\n        features: ['ID', 'sentence', 'label', 'language'],\n        num_rows: 900\n    })\n    validation: Dataset({\n        features: ['ID', 'sentence', 'label', 'language'],\n        num_rows: 100\n    })\n})\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "DoL9e1YxpaWR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def format_prompt(example):\n",
        "    prompt = f\"Classify the sentiment of the following language {example['language']} text: '{example['text']}' \\n Sentiment:\"\n",
        "    return {\"prompt\": prompt, \"labels\": example[\"label\"]}\n",
        "\n",
        "tokenized_datasets = dataset.map(format_prompt)\n",
        "tokenized_datasets = tokenized_datasets.remove_columns([\"text\", \"id\", \"language\"])\n",
        "tokenized_datasets.set_format(\"torch\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T09:53:25.960418Z",
          "iopub.execute_input": "2025-02-17T09:53:25.960632Z",
          "iopub.status.idle": "2025-02-17T09:53:25.969381Z",
          "shell.execute_reply.started": "2025-02-17T09:53:25.960614Z",
          "shell.execute_reply": "2025-02-17T09:53:25.968735Z"
        },
        "id": "E_yND3NgpaWR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T09:53:25.970433Z",
          "iopub.execute_input": "2025-02-17T09:53:25.970714Z",
          "iopub.status.idle": "2025-02-17T09:53:25.981921Z",
          "shell.execute_reply.started": "2025-02-17T09:53:25.970678Z",
          "shell.execute_reply": "2025-02-17T09:53:25.9812Z"
        },
        "id": "GO99maaypaWR",
        "outputId": "08b3342f-d29a-4380-bc47-b72d66f3be06"
      },
      "outputs": [
        {
          "execution_count": 72,
          "output_type": "execute_result",
          "data": {
            "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['ID', 'sentence', 'label', 'language', 'prompt'],\n        num_rows: 900\n    })\n    validation: Dataset({\n        features: ['ID', 'sentence', 'label', 'language', 'prompt'],\n        num_rows: 100\n    })\n})"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train'][:1]['prompt']"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T09:53:25.982849Z",
          "iopub.execute_input": "2025-02-17T09:53:25.98314Z",
          "iopub.status.idle": "2025-02-17T09:53:25.998623Z",
          "shell.execute_reply.started": "2025-02-17T09:53:25.983108Z",
          "shell.execute_reply": "2025-02-17T09:53:25.997879Z"
        },
        "id": "cupmJDyFpaWR",
        "outputId": "9313d684-73d1-4ad9-d2af-0d9643bcd1da"
      },
      "outputs": [
        {
          "execution_count": 73,
          "output_type": "execute_result",
          "data": {
            "text/plain": "[\"Below is a sentence in an Indian language. Classify its sentiment.\\nSentence: 'à¨®à©ˆà¨‚ à¨¬à¨¹à©à¨¤ à¨²à©°à¨¬à©‡ à¨¸à¨®à©‡à¨‚ à¨¤à©‹à¨‚ à¨²à©ˆà¨•à¨®à©‡ à¨‰à¨¤à¨ªà¨¾à¨¦à¨¾à¨‚ à¨¦à©€ à¨µà¨°à¨¤à©‹à¨‚ à¨•à¨° à¨°à¨¿à¨¹à¨¾/à¨°à¨¹à©€ à¨¹à¨¾à¨‚ à¨…à¨¤à©‡ à¨‡à¨¹ à¨®à©‡à¨°à©€ à¨®à©‡à¨•-à¨…à©±à¨ª à¨•à¨¿à©±à¨Ÿ à¨µà¨¿à©±à¨š à¨¹à©à¨£à©‡ à¨†à¨‡à¨† à¨¹à©ˆ à¨œà©‹ à¨‡à¨¸à¨¦à©€ à¨¬à©à¨°à¨¾à¨‚à¨¡ à¨‡à¨®à©‡à¨œ à¨…à¨¨à©à¨¸à¨¾à¨° à¨–à¨°à¨¾ à¨‰à¨¤à¨°à¨¦à¨¾ à¨¹à©ˆà¥¤ à¨‡à¨¹ à¨®à©‡à¨°à©€ à¨šà¨®à©œà©€ à¨¦à©‡ à¨•à¨¾à¨²à©‡ à¨˜à©‡à¨°à¨¿à¨†à¨‚ à¨¨à©‚à©° à¨¢à©±à¨•à¨¦à¨¾ à¨¹à©ˆ, à¨‡à¨¸ à¨¨à©‚à©° à¨šà¨®à¨•à¨¦à¨¾à¨° à¨¬à¨£à¨¾à¨‰à¨‚à¨¦à¨¾ à¨¹à©ˆ à¨…à¨¤à©‡ à¨‡à¨•à¨¸à¨¾à¨° à¨Ÿà©‹à¨¨ à¨¦à¨¿à©°à¨¦à¨¾à¥¤'\\nSentiment:\"]"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Available VRAM:\", torch.cuda.get_device_properties(0).total_memory / 1e9, \"GB\")\n",
        "print(\"Torch Reserved Memory:\", torch.cuda.memory_reserved() / 1e9, \"GB\")\n",
        "print(\"Torch Allocated Memory:\", torch.cuda.memory_allocated() / 1e9, \"GB\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T09:53:25.999601Z",
          "iopub.execute_input": "2025-02-17T09:53:25.999896Z",
          "iopub.status.idle": "2025-02-17T09:53:26.015566Z",
          "shell.execute_reply.started": "2025-02-17T09:53:25.999875Z",
          "shell.execute_reply": "2025-02-17T09:53:26.014766Z"
        },
        "id": "RreuUBILpaWR",
        "outputId": "aa80abd4-c6ab-48ad-8cd3-76905c852e99"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Available VRAM: 15.828320256 GB\nTorch Reserved Memory: 11.549016064 GB\nTorch Allocated Memory: 11.435756032 GB\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Ensure padding is handled"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T09:53:26.016318Z",
          "iopub.execute_input": "2025-02-17T09:53:26.016622Z",
          "iopub.status.idle": "2025-02-17T09:53:26.534146Z",
          "shell.execute_reply.started": "2025-02-17T09:53:26.0166Z",
          "shell.execute_reply": "2025-02-17T09:53:26.533234Z"
        },
        "id": "L1rBbWj-paWS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# LoRA Configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=8,  # Low-rank dimension\n",
        "    lora_alpha=16,  # Scaling factor\n",
        "    lora_dropout=0.1,  # Prevents overfitting\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],  # Optimizes attention layers\n",
        "    # bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# Apply LoRA\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "# trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "# total_params = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "# print(f\"Trainable Parameters: {trainable_params:,}\")\n",
        "# print(f\"Total Parameters: {total_params:,}\")\n",
        "# print(f\"Percentage of Trainable Parameters: {100 * trainable_params / total_params:.4f}%\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T09:53:26.535022Z",
          "iopub.execute_input": "2025-02-17T09:53:26.535312Z",
          "iopub.status.idle": "2025-02-17T09:53:26.654875Z",
          "shell.execute_reply.started": "2025-02-17T09:53:26.535283Z",
          "shell.execute_reply": "2025-02-17T09:53:26.654154Z"
        },
        "id": "Bg8fW3PUpaWS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "# Tokenization Function\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"prompt\"], padding=\"max_length\", truncation=True, max_length=256)\n",
        "\n",
        "# Tokenize Dataset\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Data Collator for Padding & Masking\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False  # We are doing causal LM fine-tuning\n",
        ")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T09:53:26.655648Z",
          "iopub.execute_input": "2025-02-17T09:53:26.655969Z",
          "iopub.status.idle": "2025-02-17T09:53:27.037085Z",
          "shell.execute_reply.started": "2025-02-17T09:53:26.655937Z",
          "shell.execute_reply": "2025-02-17T09:53:27.036425Z"
        },
        "id": "H3yaH1Y8paWS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_train_batch_size=1,  # Batch size for each GPU\n",
        "    per_device_eval_batch_size=1,   # Batch size for evaluation\n",
        "    gradient_accumulation_steps=8,  # Simulates batch_size=16\n",
        "    evaluation_strategy=\"steps\",    # Evaluate after each set of steps\n",
        "    eval_steps=500,                 # Evaluate every 500 steps (adjust as needed)\n",
        "    save_strategy=\"steps\",         # Save model at each evaluation step\n",
        "    save_steps=500,                # Save checkpoint every 500 steps\n",
        "    save_total_limit=2,            # Keep only the 2 most recent checkpoints\n",
        "    num_train_epochs=1,            # Number of epochs to train\n",
        "    logging_dir=\"./logs\",          # Log directory for training\n",
        "    logging_steps=100,             # Log every 100 steps\n",
        "    report_to=\"none\",              # Disable reporting to other platforms\n",
        "    fp16=True,                     # Enable mixed precision\n",
        "    optim=\"adamw_torch\",           # Optimizer choice\n",
        "    lr_scheduler_type=\"cosine\",    # Cosine decay for better convergence\n",
        "    warmup_ratio=0.03,             # Warm-up ratio\n",
        "    weight_decay=0.01,             # Regularization\n",
        "    load_best_model_at_end=True,   # Load the best model based on eval loss\n",
        "    push_to_hub=False              # Disable model uploading\n",
        ")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T09:53:27.037853Z",
          "iopub.execute_input": "2025-02-17T09:53:27.038128Z",
          "iopub.status.idle": "2025-02-17T09:53:27.073393Z",
          "shell.execute_reply.started": "2025-02-17T09:53:27.038101Z",
          "shell.execute_reply": "2025-02-17T09:53:27.072547Z"
        },
        "id": "IdaDfGEKpaWS",
        "outputId": "83b2db70-3a04-485b-ae1d-7fee4e85b51a"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if GPU is available\n",
        "print(\"CUDA Available:\", torch.cuda.is_available())  # Should print True if GPU is available\n",
        "print(\"Using Device:\", torch.cuda.get_device_name(0))  # Should print the GPU model (A100, T4, etc.)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T09:53:27.074436Z",
          "iopub.execute_input": "2025-02-17T09:53:27.074758Z",
          "iopub.status.idle": "2025-02-17T09:53:27.080052Z",
          "shell.execute_reply.started": "2025-02-17T09:53:27.074728Z",
          "shell.execute_reply": "2025-02-17T09:53:27.079189Z"
        },
        "id": "7mcZ6079paWT",
        "outputId": "dea21cd4-8200-4c90-b3d0-af25f8146fb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "CUDA Available: True\nUsing Device: Tesla T4\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "# Start Training with Step-wise Evaluation\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T09:53:27.080962Z",
          "iopub.execute_input": "2025-02-17T09:53:27.081213Z",
          "iopub.status.idle": "2025-02-17T09:53:28.274289Z",
          "shell.execute_reply.started": "2025-02-17T09:53:27.08118Z",
          "shell.execute_reply": "2025-02-17T09:53:28.272875Z"
        },
        "id": "4XVEU2RHpaWT"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}